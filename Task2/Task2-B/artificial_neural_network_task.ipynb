{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZaTwK7ojXr2F",
    "outputId": "0b27a96d-d11a-43e8-ab4b-87c1f01896fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cpu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXUkhkMfU4wq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
      "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
      "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values # write down here why we are using 3:-1 why these columns\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "\"\"\"\n",
    "So, the first 3 columns: row number, customer id and username are irrelevant to the prediction model as they have no inherit structure\n",
    "that the model can learn. \n",
    "\n",
    "The first colon (:) indicates that keep all rows - we don't need to delete any rows.\n",
    "The 3:-1 indicates keep all columns except the columns 0, 1, 2.\n",
    "\"\"\"\n",
    "\n",
    "print ( dataset.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "VYP9cQTWbzuI",
    "outputId": "797e7a64-9bac-436a-8c9c-94437e5e7587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n",
      "(10000, 10)\n",
      "Geography unique: ['France' 'Germany' 'Spain']\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print ( X.shape )\n",
    "\n",
    "print ( \"Geography unique:\", np.unique ( X[:, 1] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "38vKGE6Nb2RR",
    "outputId": "a815e42a-e0dd-4cb5-ab97-b17ead98fbc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6bQ0UgSU-NJ"
   },
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le5MJreAbW52"
   },
   "source": [
    "Label Encoding the \"Gender\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxVKWXxLbczC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "X[:, 2] = le.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "-M1KboxFb6OO",
    "outputId": "e2b8c7e8-0cbc-4cdf-f4eb-7f0853a00b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print ( X.shape ) # Keeping track of shape after encodings!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUxGZezpbMcb"
   },
   "source": [
    "One Hot Encoding the \"Geography\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMXC8-KMVirw"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer ( transformers = [ ( 'encoder', OneHotEncoder ( drop = 'first' ),  [ 1 ] ) ], remainder = 'passthrough' )\n",
    "\n",
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "ZcxwEon-b8nV",
    "outputId": "23a98af4-5e33-4b26-c27b-f06e3c5d2baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 619 ... 1 1 101348.88]\n",
      " [0.0 1.0 608 ... 0 1 112542.58]\n",
      " [0.0 0.0 502 ... 1 0 113931.57]\n",
      " ...\n",
      " [0.0 0.0 709 ... 0 1 42085.58]\n",
      " [1.0 0.0 772 ... 1 0 92888.52]\n",
      " [0.0 0.0 792 ... 1 0 38190.78]]\n",
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print ( X.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHol938cW8zd"
   },
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-TDt0Y_XEfc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split ( X, y, test_size = 0.2, random_state  = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RE_FcHyfV3TQ"
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViCrE00rV8Sk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train = sc.fit_transform ( x_train )\n",
    "x_test = sc.fit_transform ( x_test )\n",
    "\n",
    "print ( x_train.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zfEzkRVXIwF"
   },
   "source": [
    "## Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvdeScabXtlB"
   },
   "source": [
    "### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dtrScHxXQox"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc1): Linear(in_features=11, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ANN ( nn.Module ):\n",
    "    def __init__ ( self ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear ( 11, 20 )\n",
    "        self.fc2 = nn.Linear ( 20, 20 )\n",
    "        self.fc3 = nn.Linear ( 20, 1 )\n",
    "\n",
    "        nn.init.xavier_uniform_ ( self.fc1.weight )\n",
    "        nn.init.xavier_uniform_ ( self.fc2.weight )\n",
    "        nn.init.xavier_uniform_ ( self.fc3.weight )\n",
    "\n",
    "    def forward ( self, x ):\n",
    "        x = F.relu ( self.fc1 ( x ) )\n",
    "        x = F.relu ( self.fc2 ( x ) )\n",
    "        x = torch.sigmoid ( self.fc3 ( x ) )\n",
    "        return x\n",
    "\n",
    "ann = ANN()\n",
    "print ( ann )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT4u2S1_Y4WG"
   },
   "source": [
    "## Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GWlJChhY_ZI"
   },
   "source": [
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG3RrwDXZEaS"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam ( ann.parameters() )\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QR_G5u7ZLSM"
   },
   "source": [
    "### Training the ANN on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nHZ-LKv_ZRb3",
    "outputId": "718cc4b0-b5aa-40f0-9b20-d3d31730a531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Training loss: 0.3078401216864586; Val loss: 0.31403514444828035\n",
      "Epoch: 2; Training loss: 0.30567233383655545; Val loss: 0.317722747027874\n",
      "Epoch: 3; Training loss: 0.30452233731746675; Val loss: 0.323212109208107\n",
      "Epoch: 4; Training loss: 0.30389862865209577; Val loss: 0.32183813661336896\n",
      "Epoch: 5; Training loss: 0.3025967674329877; Val loss: 0.32247563093900683\n",
      "Epoch: 6; Training loss: 0.3010979598760605; Val loss: 0.32563634842634204\n",
      "Epoch: 7; Training loss: 0.3005891751125455; Val loss: 0.3280494573712349\n",
      "Epoch: 8; Training loss: 0.30017781458795073; Val loss: 0.32958600282669065\n",
      "Epoch: 9; Training loss: 0.29955273926258086; Val loss: 0.32940798103809354\n",
      "Epoch: 10; Training loss: 0.2985866117477417; Val loss: 0.33127622187137606\n",
      "Epoch: 11; Training loss: 0.29879594188183545; Val loss: 0.33080740660429003\n",
      "Epoch: 12; Training loss: 0.2976669857278466; Val loss: 0.33331413358449935\n",
      "Epoch: 13; Training loss: 0.29687682550400496; Val loss: 0.3321657946705818\n",
      "Epoch: 14; Training loss: 0.2970028768852353; Val loss: 0.33453051805496214\n",
      "Epoch: 15; Training loss: 0.295978027433157; Val loss: 0.33337383896112444\n",
      "Epoch: 16; Training loss: 0.29629627425223587; Val loss: 0.3377973982691765\n",
      "Epoch: 17; Training loss: 0.29473746940493584; Val loss: 0.3361272525787353\n",
      "Epoch: 18; Training loss: 0.29553446747362616; Val loss: 0.33915259659290314\n",
      "Epoch: 19; Training loss: 0.29464193802326916; Val loss: 0.33702512383461\n",
      "Epoch: 20; Training loss: 0.2939108676835895; Val loss: 0.34022556960582734\n",
      "Epoch: 21; Training loss: 0.2929200059920549; Val loss: 0.3382265138626099\n",
      "Epoch: 22; Training loss: 0.2931281604245305; Val loss: 0.3411021739244461\n",
      "Epoch: 23; Training loss: 0.29362446509301665; Val loss: 0.337119098007679\n",
      "Epoch: 24; Training loss: 0.29302256673574445; Val loss: 0.341317845582962\n",
      "Epoch: 25; Training loss: 0.29191576056182383; Val loss: 0.33976004242897034\n",
      "Epoch: 26; Training loss: 0.29194798558950424; Val loss: 0.34227269411087036\n",
      "Epoch: 27; Training loss: 0.29214395519346; Val loss: 0.34329810351133344\n",
      "Epoch: 28; Training loss: 0.29129517436027524; Val loss: 0.33858905494213104\n",
      "Epoch: 29; Training loss: 0.29152922980487345; Val loss: 0.34312098085880277\n",
      "Epoch: 30; Training loss: 0.2909298834949732; Val loss: 0.3449797722697258\n",
      "Epoch: 31; Training loss: 0.28989314906299113; Val loss: 0.3462509581446648\n",
      "Epoch: 32; Training loss: 0.2902823821827769; Val loss: 0.3423471674323082\n",
      "Epoch: 33; Training loss: 0.28991931073367594; Val loss: 0.34415571510791776\n",
      "Epoch: 34; Training loss: 0.2894308152794838; Val loss: 0.34712314456701276\n",
      "Epoch: 35; Training loss: 0.28942791823297737; Val loss: 0.34515834152698516\n",
      "Epoch: 36; Training loss: 0.2890824903547764; Val loss: 0.3497424507141113\n",
      "Epoch: 37; Training loss: 0.28850477647036316; Val loss: 0.34679525792598725\n",
      "Epoch: 38; Training loss: 0.2891969084367156; Val loss: 0.3477998593449593\n",
      "Epoch: 39; Training loss: 0.28873246025294064; Val loss: 0.34895610868930815\n",
      "Epoch: 40; Training loss: 0.28828123286366464; Val loss: 0.3492917439341545\n",
      "Epoch: 41; Training loss: 0.2873480064049363; Val loss: 0.3493312186002731\n",
      "Epoch: 42; Training loss: 0.28733930084854364; Val loss: 0.3519755038619041\n",
      "Epoch: 43; Training loss: 0.28782473158091304; Val loss: 0.3482077693939209\n",
      "Epoch: 44; Training loss: 0.28754928179085254; Val loss: 0.34709059953689575\n",
      "Epoch: 45; Training loss: 0.2872881420701742; Val loss: 0.3478411468863487\n",
      "Epoch: 46; Training loss: 0.2870470444858074; Val loss: 0.35053088903427126\n",
      "Epoch: 47; Training loss: 0.2861659798398614; Val loss: 0.3522949469089508\n",
      "Epoch: 48; Training loss: 0.28698894429951904; Val loss: 0.3509054991602898\n",
      "Epoch: 49; Training loss: 0.28630043644458053; Val loss: 0.34784920781850814\n",
      "Epoch: 50; Training loss: 0.28541398238390686; Val loss: 0.3519005200266838\n",
      "Epoch: 51; Training loss: 0.28659887090325353; Val loss: 0.3530165937542915\n",
      "Epoch: 52; Training loss: 0.28511075366288424; Val loss: 0.35083111345767976\n",
      "Epoch: 53; Training loss: 0.2850060477107763; Val loss: 0.34907892137765884\n",
      "Epoch: 54; Training loss: 0.28505374666303396; Val loss: 0.35516255229711535\n",
      "Epoch: 55; Training loss: 0.2845483887568116; Val loss: 0.3512596806883812\n",
      "Epoch: 56; Training loss: 0.2849337654933333; Val loss: 0.356985741853714\n",
      "Epoch: 57; Training loss: 0.2840224014222622; Val loss: 0.35142200827598574\n",
      "Epoch: 58; Training loss: 0.28457880090922116; Val loss: 0.35533572018146514\n",
      "Epoch: 59; Training loss: 0.2840772894769907; Val loss: 0.35481936007738113\n",
      "Epoch: 60; Training loss: 0.28398934572935103; Val loss: 0.35168677240610124\n",
      "Epoch: 61; Training loss: 0.28424516178667547; Val loss: 0.35271040081977845\n",
      "Epoch: 62; Training loss: 0.28397455614060163; Val loss: 0.35519901633262635\n",
      "Epoch: 63; Training loss: 0.28307932505384087; Val loss: 0.35534717082977296\n",
      "Epoch: 64; Training loss: 0.2835801822692156; Val loss: 0.3513758888840675\n",
      "Epoch: 65; Training loss: 0.28308771777898073; Val loss: 0.354215449988842\n",
      "Epoch: 66; Training loss: 0.28320844158530234; Val loss: 0.3508233132958412\n",
      "Epoch: 67; Training loss: 0.2829225687682629; Val loss: 0.35818111419677734\n",
      "Epoch: 68; Training loss: 0.2827810209617019; Val loss: 0.35453776389360425\n",
      "Epoch: 69; Training loss: 0.2827245303615928; Val loss: 0.35560114711523055\n",
      "Epoch: 70; Training loss: 0.2828481014072895; Val loss: 0.35529003709554674\n",
      "Epoch: 71; Training loss: 0.28147218517959116; Val loss: 0.354616157412529\n",
      "Epoch: 72; Training loss: 0.2820766903832555; Val loss: 0.3548832893371582\n",
      "Epoch: 73; Training loss: 0.2823389944806695; Val loss: 0.3536972177028656\n",
      "Epoch: 74; Training loss: 0.2813320992514491; Val loss: 0.35807893335819246\n",
      "Epoch: 75; Training loss: 0.2811245681717992; Val loss: 0.35447333186864854\n",
      "Epoch: 76; Training loss: 0.28209144007414577; Val loss: 0.3560778158903122\n",
      "Epoch: 77; Training loss: 0.28093214765191077; Val loss: 0.3561264279484749\n",
      "Epoch: 78; Training loss: 0.28143384255468845; Val loss: 0.35912264317274095\n",
      "Epoch: 79; Training loss: 0.28005278538912537; Val loss: 0.3570661926269531\n",
      "Epoch: 80; Training loss: 0.28053179126232863; Val loss: 0.359236920773983\n",
      "Epoch: 81; Training loss: 0.28049033898860215; Val loss: 0.35700765073299406\n",
      "Epoch: 82; Training loss: 0.28064090307801964; Val loss: 0.36074173092842104\n",
      "Epoch: 83; Training loss: 0.27950746603310106; Val loss: 0.36048833668231967\n",
      "Epoch: 84; Training loss: 0.28097905062139034; Val loss: 0.35765289574861525\n",
      "Epoch: 85; Training loss: 0.2791771424561739; Val loss: 0.35367044657468794\n",
      "Epoch: 86; Training loss: 0.2789484728500247; Val loss: 0.36009547173976897\n",
      "Epoch: 87; Training loss: 0.2782277234271169; Val loss: 0.35802530258893966\n",
      "Epoch: 88; Training loss: 0.2796515736915171; Val loss: 0.3572950446605682\n",
      "Epoch: 89; Training loss: 0.2794652995839715; Val loss: 0.35989834994077685\n",
      "Epoch: 90; Training loss: 0.2788373386114836; Val loss: 0.36072083592414855\n",
      "Epoch: 91; Training loss: 0.27820363704115153; Val loss: 0.3602797457575798\n",
      "Epoch: 92; Training loss: 0.2789896488189697; Val loss: 0.35940714687108993\n",
      "Epoch: 93; Training loss: 0.27814561169594526; Val loss: 0.36160294204950333\n",
      "Epoch: 94; Training loss: 0.27724456306546924; Val loss: 0.35925608575344087\n",
      "Epoch: 95; Training loss: 0.2784550031274557; Val loss: 0.36090427279472354\n",
      "Epoch: 96; Training loss: 0.2780020594969392; Val loss: 0.36132804065942764\n",
      "Epoch: 97; Training loss: 0.2779575827717781; Val loss: 0.36300988793373107\n",
      "Epoch: 98; Training loss: 0.27693128429353236; Val loss: 0.3606699073314667\n",
      "Epoch: 99; Training loss: 0.27810097094625236; Val loss: 0.36363139748573303\n",
      "Epoch: 100; Training loss: 0.27763736274093387; Val loss: 0.36668996900320056\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "X = torch.tensor ( x_train, dtype = torch.float32 )\n",
    "y = torch.tensor ( y_train, dtype = torch.float32 ).view ( -1, 1 )\n",
    "\n",
    "dataset = TensorDataset ( X, y )\n",
    "\n",
    "val_size = int ( 0.2 * len ( dataset ) )\n",
    "train_size = len ( dataset ) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split ( dataset, [ train_size, val_size ] )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range ( 100 ):\n",
    "    ann.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = ann ( xb )\n",
    "        loss_back = loss ( preds, yb )\n",
    "        loss_back.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss_back.item() * xb.size(0)\n",
    "\n",
    "    train_loss /= train_size\n",
    "\n",
    "    ann.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = ann ( xb )\n",
    "            loss_back = loss ( preds, yb )\n",
    "            val_loss += loss_back.item() * xb.size(0)\n",
    "\n",
    "    val_loss /= val_size\n",
    "\n",
    "    print ( \"Epoch: {}; Training loss: {}; Val loss: {}\".format ( str ( epoch + 1 ), str ( train_loss ), str ( val_loss ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJj5k2MxZga3"
   },
   "source": [
    "## Part 4 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84QFoqGYeXHL"
   },
   "source": [
    "### Predicting the result of a single observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGRo3eacgDdC"
   },
   "source": [
    "**Extra**\n",
    "\n",
    "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: \\$ 60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card ? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: \\$ 50000\n",
    "\n",
    "So, should we say goodbye to that customer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhU1LTgPg-kH"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2d8IoCCkeWGL",
    "outputId": "957f3970-e197-4c3b-a150-7f69dc567f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, should we say goodbye to that customer? :  True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "row_in = np.array ( [[600,'France','Male',40,3,60000,2,1,1,50000]] )\n",
    "\n",
    "row_in[:, 2] = le.fit_transform ( row_in[:, 2] )\n",
    "row_in = ct.transform ( row_in )\n",
    "row_in = sc.transform ( row_in )\n",
    "\n",
    "row_tensor = torch.tensor ( row_in, dtype = torch.float32 )\n",
    "\n",
    "ann.eval()\n",
    "with torch.no_grad():\n",
    "    prob = ann ( row_tensor ).item()\n",
    "\n",
    "print( \"So, should we say goodbye to that customer? : \", prob <= 0.5 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7yx47jPZt11"
   },
   "source": [
    "\n",
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0oyfLWoaEGw"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ci6K_r6LaF6P",
    "outputId": "4d854e9e-22d5-432f-f6e5-a102fe3ae0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Test Accuracy: 0.852\n",
      "Confusion matrix:\n",
      " [[1525   82]\n",
      " [ 214  179]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      1607\n",
      "           1       0.69      0.46      0.55       393\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.78      0.70      0.73      2000\n",
      "weighted avg       0.84      0.85      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "y_pred = ann.predict ( x_test )\n",
    "y_pred = ( y_pred > 0.5 )\n",
    "\n",
    "print ( \"Test Accuracy:\", accuracy_score ( y_test, y_pred ) )\n",
    "\n",
    "con_matr = confusion_matrix ( y_test, y_pred )\n",
    "print ( \"Confusion matrix:\\n\", con_matr )\n",
    "\n",
    "print ( \"Classification Report:\\n\", classification_report ( y_test, y_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write down about precision recall f1-score, why is it better than just accuracy, what are some other interesting metrics u can find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = TP / ( TP + FP )\n",
    "TP - True positive, FP - False positive\n",
    "Precision basically tells us out of all +ves predicted, how many were actually positive. So a high precision means less false positives.\n",
    "\n",
    "Recall = TP / ( TP + FN )\n",
    "FN - False negative\n",
    "Recall tells us, out of all actual positive test cases how many the model correctly identified. High recall => Less false negatives.\n",
    "\n",
    "F1 score = harmonic mean ( Precision, Recall ). Its like a balance for pr:ecision and recall. A model cannot cheat by having just high recall/precision.\n",
    "\n",
    "The problem with accuracy is that if the test cases are highly imbalanced towards one class, say 80% is zero class. Then a model that predicts everything as zero will have 80% accurac, which is not a good metric.\n",
    "\n",
    "There is also a metric called specificity - how well a model can identify negatives.\n",
    "specificity = TN / ( TN + FP )\n",
    "TN - true negative"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
